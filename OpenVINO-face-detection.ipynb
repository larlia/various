{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVINO model setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "import cv2\n",
    "import time\n",
    "import logging as log\n",
    "from openvino.inference_engine import IENetwork, IEPlugin\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Class setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class OpenVinoObjectDetectionModel(object):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Builds an OpenVINO model.\n",
    "\n",
    "        Keyword arguments (in order):\n",
    "        model_path: Path to an .xml file with a trained model.\n",
    "        cpu_extension: MKLDNN (CPU)-targeted custom layers. Absolute path to a shared library with the kernels impl.\n",
    "        plugin_dir: Path to a plugin folder\n",
    "        device: Specify the target device to infer on; CPU, GPU, FPGA or MYRIAD is acceptable. CPU by default.\n",
    "        labels_path: Labels mapping file (format .labels)\n",
    "        prob_threshold: Probability threshold for detections filtering. Float between 0.0 and 1.0.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.__dict__.update(kwargs)\n",
    "        log.basicConfig(format=\"[ %(levelname)s ] %(message)s\", level=log.INFO, stream=sys.stdout)\n",
    "        self.generate(**kwargs)\n",
    "        log.info(\"Model initialized and loaded.\")\n",
    "\n",
    "    def generate(self, model_path, cpu_extension = None, plugin_dir = None, device = \"CPU\",\n",
    "                labels_path = None, prob_threshold = 0.5):\n",
    "\n",
    "        self.model_xml = model_path\n",
    "        self.model_bin = os.path.splitext(self.model_xml)[0] + \".bin\"\n",
    "\n",
    "        # Plugin initialization for specified device and load extensions library if specified\n",
    "        log.info(\"Initializing plugin for {} device...\".format(device))\n",
    "        self.plugin = IEPlugin(device=device, plugin_dirs=plugin_dir)\n",
    "        if cpu_extension and 'CPU' in device:\n",
    "            self.plugin.add_cpu_extension(cpu_extension)\n",
    "\n",
    "        # Read IR\n",
    "        log.info(\"Reading IR...\")\n",
    "        self.net = IENetwork.from_ir(model=self.model_xml, weights=self.model_bin)\n",
    "\n",
    "        if \"CPU\" in self.plugin.device:\n",
    "            supported_layers = self.plugin.get_supported_layers(self.net)\n",
    "            not_supported_layers = [l for l in self.net.layers.keys() if l not in supported_layers]\n",
    "            if len(not_supported_layers) != 0:\n",
    "                log.error(\"Following layers are not supported by the plugin for specified device {}:\\n {}\".\n",
    "                          format(self.plugin.device, ', '.join(not_supported_layers)))\n",
    "                log.error(\"Please try to specify cpu extensions library path in sample's command line parameters using -l \"\n",
    "                          \"or --cpu_extension command line argument\")\n",
    "                raise ValueError(\"Some layers are not supported by the plugin for the specified device {}\".format(device))\n",
    "\n",
    "        assert len(self.net.inputs.keys()) == 1, \"Sample supports only single input topologies\"\n",
    "        assert len(self.net.outputs) == 1, \"Sample supports only single output topologies\"\n",
    "        self.input_blob = next(iter(self.net.inputs))\n",
    "        self.out_blob = next(iter(self.net.outputs))\n",
    "        log.info(\"Loading IR to the plugin...\")\n",
    "        self.exec_net = self.plugin.load(network=self.net, num_requests=2)\n",
    "        self.cur_request_id = 0\n",
    "        self.next_request_id = 1\n",
    "        \n",
    "        self.n, self.c, self.h, self.w = self.net.inputs[self.input_blob].shape\n",
    "        del self.net\n",
    "\n",
    "    def detect_objects(self, image, resolution):\n",
    "        \"\"\"\n",
    "        Runs inference on the supplied image.\n",
    "\n",
    "        Keyword arguments:\n",
    "        image: Image to be inferenced on\n",
    "        resolution: Tuple of (width, height) of the image\n",
    "        \"\"\"\n",
    "        is_async_mode = False\n",
    "        \n",
    "        image = cv2.resize(image, (self.w, self.h))\n",
    "        image = image.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "        image = image.reshape((self.n, self.c, self.h, self.w))\n",
    "\n",
    "        # Main sync point:\n",
    "        # in the truly Async mode we start the NEXT infer request, while waiting for the CURRENT to complete\n",
    "        # in the regular mode we start the CURRENT request and immediately wait for it's completion\n",
    "\n",
    "        if is_async_mode:\n",
    "            self.exec_net.start_async(request_id=self.next_request_id, inputs={self.input_blob: image})\n",
    "        else:\n",
    "            self.exec_net.start_async(request_id=self.cur_request_id, inputs={self.input_blob: image})\n",
    "            \n",
    "        if self.exec_net.requests[self.cur_request_id].wait(-1) == 0:\n",
    "\n",
    "            # Parse detection results of the current request\n",
    "            res = self.exec_net.requests[self.cur_request_id].outputs[self.out_blob]\n",
    "            bboxes = []\n",
    "            \n",
    "            for obj in res[0][0]:\n",
    "                # Draw only objects when probability more than specified threshold\n",
    "                confidence = obj[2]\n",
    "                if confidence > self.prob_threshold:\n",
    "                    xmin = int(obj[3] * resolution[0])\n",
    "                    ymin = int(obj[4] * resolution[1])\n",
    "                    xmax = int(obj[5] * resolution[0])\n",
    "                    ymax = int(obj[6] * resolution[1])\n",
    "                    class_id = int(obj[1])\n",
    "                    bboxes.append((class_id, confidence, xmin, ymin, xmax, ymax))\n",
    "        \n",
    "        self.next_request_id, self.cur_request_id = self.cur_request_id, self.next_request_id\n",
    "        return image, bboxes\n",
    "    \n",
    "    def detect_objects_partition(self, subimages, partition):\n",
    "        \"\"\"\n",
    "        Runs inference on the supplied subimages. Returns bboxes with original image coordinates\n",
    "\n",
    "        Keyword arguments:\n",
    "        subimages: List of images to be inferenced on\n",
    "        partition: Dict with keys (\"xmins\", \"xmaxs\", \"ymins\", \"ymaxs\"), each of which contains \n",
    "                   a list of coordinates corresponding to the subimages        \n",
    "        \"\"\"\n",
    "        bboxes = []\n",
    "        \n",
    "        for i, image in enumerate(subimages):\n",
    "            image = cv2.resize(image, (self.w, self.h))\n",
    "            image = image.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "            image = image.reshape((self.n, self.c, self.h, self.w))\n",
    "\n",
    "            self.exec_net.start_async(request_id = 0, inputs = {self.input_blob: image})\n",
    "                \n",
    "            if self.exec_net.requests[0].wait(-1) == 0:\n",
    "                \n",
    "                part_height = partition[\"ymaxs\"][i] - partition[\"ymins\"][i]\n",
    "                part_width = partition[\"xmaxs\"][i] - partition[\"xmins\"][i]\n",
    "                \n",
    "                # Parse detection results of the current request\n",
    "                res = self.exec_net.requests[self.cur_request_id].outputs[self.out_blob]\n",
    "                    \n",
    "                for obj in res[0][0]:\n",
    "                    # Draw only objects when probability more than specified threshold\n",
    "                    confidence = obj[2]\n",
    "                    if confidence > self.prob_threshold:\n",
    "                        xmin = int(obj[3] * part_width) + partition[\"xmins\"][i]\n",
    "                        ymin = int(obj[4] * part_height) + partition[\"ymins\"][i]\n",
    "                        xmax = int(obj[5] * part_width) + partition[\"xmins\"][i]\n",
    "                        ymax = int(obj[6] * part_height) + partition[\"ymins\"][i]\n",
    "                        class_id = int(obj[1])\n",
    "                        bboxes.append((class_id, confidence, xmin, ymin, xmax, ymax))\n",
    "        \n",
    "        return bboxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection and tracking model setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Class setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     101,
     124,
     137
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SafeHomeDetector(object):\n",
    "    \n",
    "    def __init__(self, model, classes, drawer, resize_resolution = None):\n",
    "        \n",
    "        self.timer = Timer()\n",
    "        self.timer.start_task(\"Setup\")\n",
    "        \n",
    "        self.model = model\n",
    "        self.classes = classes\n",
    "        self.drawer = drawer\n",
    "        \n",
    "        if resize_resolution is not None:\n",
    "            self.resolution = resize_resolution\n",
    "            self.resize_resolution = resize_resolution\n",
    "        else:\n",
    "            self.resolution = None\n",
    "        \n",
    "        self.model_name = \"OpenVINO Object Detection\"\n",
    "        \n",
    "    def execute(self, vid_source, output_path = None):\n",
    "        \"\"\"\n",
    "        Starts a session of detection and tracking on the provided video using provided data.\n",
    "        \n",
    "        Keyword arguments:\n",
    "        vid_source:         String. Full path to video or video stream\n",
    "        output_path:        String. Full path to file where the output will be saved. \n",
    "                            If not provided, no video will be saved.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._initialize_run(vid_source, output_path)\n",
    "        \n",
    "        # Initialize time variables to measure FPS\n",
    "        time_start = time.time()\n",
    "        time_this = time.time()\n",
    "        time_prev = time.time()\n",
    "\n",
    "        # ret will become false when all frames in the input video has been read\n",
    "        ret = True\n",
    "\n",
    "        while (ret):\n",
    "\n",
    "            self.timer.start_task(\"Video/image processing\")\n",
    "\n",
    "            # Get next frame from video\n",
    "            ret, self.current_image = self.cap.read()\n",
    "\n",
    "            # If there are no more frames, exit the loop \n",
    "            if ret == False:\n",
    "                break\n",
    "\n",
    "            # Resize if needed\n",
    "            if self.resize_resolution is not None:\n",
    "                self.current_image = cv2.resize(self.current_image, self.resolution)\n",
    "\n",
    "            # self.current_image_tracking = cv2.resize(self.current_image, self.tracking_resolution)\n",
    "\n",
    "            self.counters[\"frames\"] += 1\n",
    "\n",
    "            # self.timer.start_task(\"Managing trackers\")\n",
    "\n",
    "            # Update trackers and boxes\n",
    "            # self._update_trackers()\n",
    "\n",
    "            # Do detection\n",
    "            self.timer.start_task(\"Detection\")\n",
    "            _, bboxes_detected = self.model.detect_objects(self.current_image, self.resolution)\n",
    "\n",
    "            # self.timer.start_task(\"Processing detection\")\n",
    "            # self._process_detections(bboxes_detected)\n",
    "\n",
    "            self.timer.start_task(\"Drawing on image\")\n",
    "            \n",
    "            # Calculate FPS \n",
    "            time_this = time.time()\n",
    "            fps = 1 / (time_this - time_prev + 0.00001) \n",
    "            time_prev = time.time()\n",
    "            \n",
    "            # Draw overlay\n",
    "            self.current_image = self.drawer.draw_overlay(self.current_image, fps)\n",
    "\n",
    "            # Draw bboxes on the image\n",
    "            self.current_image = self.drawer.draw_bboxes_on_image(self.current_image, \n",
    "                                                                  bboxes_detected, self.classes)\n",
    "\n",
    "            # Clean up finished vehicles\n",
    "            # self.trackers = [tracker for n, tracker in enumerate(self.trackers) if tracker[\"finished\"] == False]\n",
    "\n",
    "            self.timer.start_task(\"Video display\")\n",
    "            \n",
    "            # Write frame to output if applicable\n",
    "            if self.output_path is not None:\n",
    "                self.video_output.write(self.current_image)\n",
    "\n",
    "            # Show output in window\n",
    "            cv2.imshow(self.model_name, self.current_image)\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord(\"q\"):\n",
    "                break\n",
    "                \n",
    "        self._finalize_run()\n",
    "\n",
    "    def _initialize_run(self, vid_source, output_path):\n",
    "        self.output_path = output_path\n",
    "        \n",
    "        self.counters = {\n",
    "            \"n_vehicles\": 0,\n",
    "            \"lost_trackers\": 0,\n",
    "            \"frames\": 0,\n",
    "        }\n",
    "            \n",
    "        self.cap = cv2.VideoCapture(vid_source)\n",
    "        \n",
    "        # If an output path is specified, create a video output with the same attributes\n",
    "        # as the input video\n",
    "        framerate = self.cap.get(cv2.CAP_PROP_FPS)\n",
    "        \n",
    "        if self.resolution is None:\n",
    "            self.resolution = (int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "\n",
    "        if self.output_path is not None:\n",
    "            codec = cv2.VideoWriter_fourcc('m','p','4','v') # fourcc stands for four character code\n",
    "            self.video_output = cv2.VideoWriter(self.output_path, codec, framerate, self.resolution)\n",
    "        self.timer.stop()\n",
    "     \n",
    "    def _finalize_run(self):\n",
    "        cv2.destroyAllWindows()\n",
    "        self.cap.release()\n",
    "        if self.output_path is not None:\n",
    "            self.video_output.release()\n",
    "\n",
    "        self.timer.stop()\n",
    "        timers = self.timer.get_timers()\n",
    "\n",
    "        print(\"\\n======== Timer summary ========\")\n",
    "        for key in timers:\n",
    "            print(\"{}:{} {:0.4f} seconds \\t({:0.4f} s per frame)\".format(key, \" \" * (25-len(key)), timers[key], timers[key] / self.counters[\"frames\"]))\n",
    "    \n",
    "    def _get_iou(self, box1, box2):\n",
    "        \"\"\"Returns the intersection over union (IoU) between box1 and box2\n",
    "    \n",
    "        Arguments:\n",
    "        box1 -- first box, list object with coordinates (x1, y1, x2, y2)\n",
    "        box2 -- second box, list object with coordinates (x1, y1, x2, y2)\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate the (y1, x1, y2, x2) coordinates of the intersection of box1 and box2. Calculate its Area.\n",
    "        xi1 = max(box1[0], box2[0])\n",
    "        yi1 = max(box1[1], box2[1])\n",
    "        xi2 = min(box1[2], box2[2])\n",
    "        yi2 = min(box1[3], box2[3])\n",
    "        inter_area = (xi2 - xi1) * (yi2 - yi1)\n",
    "\n",
    "        # Calculate the Union area by using Formula: Union(A,B) = A + B - Inter(A,B)\n",
    "        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "        # compute the IoU\n",
    "        iou = inter_area / union_area\n",
    "\n",
    "        return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions and classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Drawer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class OverlayDrawer(object):\n",
    "    \n",
    "    def __init__(self, num_classes, colors = None):\n",
    "        self.font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        self.font_scale_small = 0.5\n",
    "        self.font_scale_large = 1.0\n",
    "        self.thickness_small = 1\n",
    "        self.thickness_medium = 2\n",
    "        self.thickness_large = 4\n",
    "        self.padding = 4\n",
    "        if colors == None:\n",
    "            self.colors = plt.cm.hsv(np.linspace(0, 1, num_classes)).tolist()\n",
    "            self.colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), self.colors))\n",
    "        elif len(colors) != num_classes:\n",
    "            print(\"Number of colors is not equal to number of classes.\")\n",
    "            self.colors = plt.cm.hsv(np.linspace(0, 1, num_classes)).tolist()\n",
    "            self.colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), self.colors))\n",
    "        else:\n",
    "            self.colors = colors\n",
    "            \n",
    "        # Some standard colors to use\n",
    "        self.c_fps = (30, 255, 20) # Green\n",
    "        self.c_white = (255, 255, 255) # White\n",
    "        \n",
    "    def draw_bboxes_on_image(self, image, bboxes, classes):\n",
    "        for n, bbox in enumerate(bboxes):\n",
    "            classification = int(bbox[0])\n",
    "            confidence = bbox[1]\n",
    "            left, top, right, bottom = bbox[2], bbox[3], bbox[4], bbox[5]\n",
    "            \n",
    "            color = self.colors[classification]\n",
    "            label = \"{} {:.2f}\".format(classes[classification], confidence)\n",
    "\n",
    "            # Draw main box\n",
    "            image = cv2.rectangle(image, (left, top), (right, bottom), color, 2)\n",
    "\n",
    "            # Draw label box above the top left corner of the main box\n",
    "            label_size = cv2.getTextSize(label, self.font, self.font_scale_small, self.thickness_small)\n",
    "            label_width = int(label_size[0][0])\n",
    "            label_height = int(label_size[0][1])\n",
    "\n",
    "            # If there is space above the box, draw the label there; else draw it below\n",
    "            if top - label_height > 0:\n",
    "                label_top = top - label_height - self.padding\n",
    "            else:\n",
    "                label_top = bottom\n",
    "\n",
    "            label_bottom = label_top + label_height + self.padding\n",
    "            label_left = left\n",
    "            label_right = left + label_width + self.padding\n",
    "            image = cv2.rectangle(image, (label_left, label_top), (label_right, label_bottom), color, -1)\n",
    "            image = cv2.putText(image, label, \n",
    "                                (label_left + int(self.padding * 0.5), label_bottom - int(self.padding * 0.5)),\n",
    "                                self.font, self.font_scale_small, self.c_white, self.thickness_small)\n",
    "\n",
    "        return image\n",
    "    \n",
    "    def draw_overlay(self, image, fps):\n",
    "        resolution = (image.shape[1], image.shape[0])\n",
    "                \n",
    "        # Add FPS\n",
    "        cv2.putText(image, \"FPS: {:.2f}\".format(fps), (3, 25), self.font, self.font_scale_large, \n",
    "                    self.c_fps, self.thickness_medium)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Timer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "class Timer(object):\n",
    "    # Helper class to time task. Every tile start(task) is called, the previous task is stopped and recorded\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._timers = {}\n",
    "        self._time_now = time.time()\n",
    "        self._time_prev = time.time()\n",
    "        self._curr_task = None\n",
    "        \n",
    "    def start_task(self, task):\n",
    "        self._time_now = time.time()\n",
    "        time_passed = self._time_now - self._time_prev\n",
    "        if self._curr_task is not None:\n",
    "            self._timers[self._curr_task] = self._timers.get(self._curr_task, 0.0) + time_passed\n",
    "        self._curr_task = task\n",
    "        self._time_prev = time.time()\n",
    "        \n",
    "    def stop(self):\n",
    "        self._time_now = time.time()\n",
    "        time_passed = self._time_now - self._time_prev\n",
    "        if self._curr_task is not None:\n",
    "            self._timers[self._curr_task] = self._timers.get(self._curr_task, 0.0) + time_passed\n",
    "        self._curr_task = None\n",
    "            \n",
    "    def get_timers(self):\n",
    "        return self._timers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Fixed parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Fixed parameters. Normally do not need to be changed unless adding new models ###\n",
    "\n",
    "# Should be validated when running on a new installation of OpenVINO\n",
    "openvino_dir = \"C:/Intel/computer_vision_sdk_2018.4.420\"\n",
    "\n",
    "# Class names\n",
    "classes_mscoco = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "           'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "           'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
    "           'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
    "           'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "           'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "           'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "           'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
    "           'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "           'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "           'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
    "           'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "           'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
    "           'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
    "           'teddy bear', 'hair drier', 'toothbrush']\n",
    "classes_intel = [\"bg\", \"face\", \"unknown\"]\n",
    "\n",
    "# Device management\n",
    "data_type = {\"CPU\": \"FP32\", \"GPU\": \"FP16\", \"MYRIAD\": \"FP16\"}\n",
    "\n",
    "models = [\"Intel face detection\",\n",
    "          \"TF SSD MobileNet v2\", \n",
    "          \"TF SSD MobileNet FPN\",\n",
    "          \"TF SSDlite MobileNet v2\"]\n",
    "model_paths = {\"Intel face detection\": (\"deployment_tools/intel_models/face-detection-adas-0001\",\"face-detection-adas-0001.xml\"),\n",
    "          \"TF SSD MobileNet v2\": (\"deployment_tools/custom_models/tensorflow_ssd_mobilenet_v2_coco\",\"ssd_mobilenet_v2_coco.xml\"),\n",
    "          \"TF SSD MobileNet FPN\": (\"deployment_tools/custom_models/ssd_mobilenet_v1_fpn_640x640\",\"ssd_mobilenet_v1_fpn_640x640.xml\"),\n",
    "          \"TF SSDlite MobileNet v2\": (\"deployment_tools/custom_models/ssdlite_mobilenet_v2\", \"ssdlite_mobilenet_v2.xml\")    \n",
    "           }\n",
    "classes = {\"Intel face detection\": classes_intel,\n",
    "          \"TF SSD MobileNet v2\": classes_mscoco,\n",
    "          \"TF SSD MobileNet FPN\": classes_mscoco,\n",
    "          \"TF SSDlite MobileNet v2\": classes_mscoco    \n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Free parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_choice = models[0]\n",
    "cpu_extension = \"C:/Intel/computer_vision_sdk_2018.3.343/deployment_tools/inference_engine/bin/intel64/Release/cpu_extension.dll\"\n",
    "plugin_dir = None\n",
    "device = \"CPU\" # Options are CPU, GPU (may require updated GPU drivers) and MYRIAD (requires Movidius NCS)\n",
    "labels_path = None \n",
    "prob_threshold = 0.5 # Confidence threshold for detections - all detection with lower confidence will be discarded\n",
    "\n",
    "video_resolution = (1280, 720) # Video will be resized to this size. Set to None for no resizing\n",
    "vid_source = \"input/burglary.mp4\" # Path to source video\n",
    "output_path = None # If a path is provided (including file name .avi), it will write the output as a video file in the given path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] Initializing plugin for CPU device...\n",
      "[ INFO ] Reading IR...\n",
      "[ INFO ] Loading IR to the plugin...\n",
      "[ INFO ] Model initialized and loaded.\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model_path = \"{}/{}/{}/{}\".format(openvino_dir, model_paths[model_choice][0], data_type[device], model_paths[model_choice][1])\n",
    "model = OpenVinoObjectDetectionModel(model_path = model_path, \n",
    "                                     cpu_extension = cpu_extension, \n",
    "                                     plugin_dir = plugin_dir, \n",
    "                                     device = device,\n",
    "                                     labels_path = labels_path, \n",
    "                                     prob_threshold = prob_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize drawer object\n",
    "drawer = OverlayDrawer(len(classes[model_choice]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize controller class\n",
    "program = SafeHomeDetector(model = model,\n",
    "                           classes = classes[model_choice], \n",
    "                           drawer = drawer,\n",
    "                           resize_resolution = video_resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Timer summary ========\n",
      "Setup:                     0.5392 seconds \t(0.0009 s per frame)\n",
      "Video/image processing:    5.1763 seconds \t(0.0087 s per frame)\n",
      "Detection:                 21.1673 seconds \t(0.0354 s per frame)\n",
      "Drawing on image:          0.1209 seconds \t(0.0002 s per frame)\n",
      "Video display:             2.7487 seconds \t(0.0046 s per frame)\n"
     ]
    }
   ],
   "source": [
    "# Execute program\n",
    "program.execute(vid_source = vid_source, output_path = output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
